name: Nigerian Real Estate Scraper

on:
  # Trigger via frontend (repository_dispatch)
  repository_dispatch:
    types: [trigger-scrape]

  # Manual trigger from GitHub Actions UI (you control when to run)
  workflow_dispatch:
    inputs:
      page_cap:
        description: 'Max pages per site'
        required: false
        default: '20'
      geocode:
        description: 'Enable geocoding (1=yes, 0=no)'
        required: false
        default: '1'
      sites:
        description: 'Specific sites to scrape (comma-separated, leave empty for all enabled sites)'
        required: false
        default: ''

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # Increased from 60 to 180 minutes (3 hours)

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright and browsers
        run: |
          playwright install chromium
          playwright install-deps chromium

      - name: Configure scraper
        run: |
          # Use config.yaml from repository
          if [ ! -f config.yaml ]; then
            echo "ERROR: config.yaml not found"
            exit 1
          fi
          echo "Using config.yaml from repository"

      - name: Run scraper
        env:
          RP_PAGE_CAP: ${{ github.event.inputs.page_cap || github.event.client_payload.page_cap || '20' }}
          RP_GEOCODE: ${{ github.event.inputs.geocode || github.event.client_payload.geocode || '1' }}
          RP_HEADLESS: 1
          RP_NO_IMAGES: 1
          RP_DEBUG: 0
        run: |
          echo "Configuration:"
          echo "  - Page cap: $RP_PAGE_CAP"
          echo "  - Geocoding: $RP_GEOCODE"
          echo "  - Headless: $RP_HEADLESS"
          echo ""
          echo "Starting scraper..."
          python main.py

      - name: Process exports with watcher
        run: |
          echo "Processing scraped data..."
          python watcher.py --once

      - name: Upload to Firestore
        env:
          FIREBASE_CREDENTIALS: ${{ secrets.FIREBASE_CREDENTIALS }}
        run: |
          echo "Uploading data to Firestore..."

          # Create temporary credentials file from GitHub secret
          echo "$FIREBASE_CREDENTIALS" > firebase-temp-credentials.json

          # Set environment variable for the script
          export FIREBASE_SERVICE_ACCOUNT=firebase-temp-credentials.json

          # Run Firestore upload with cleanup enabled
          # Archives stale listings (>30 days old) to properties_archive collection
          python scripts/upload_to_firestore.py --cleanup --max-age-days 30

          # Clean up credentials file
          rm firebase-temp-credentials.json

          echo "Firestore upload complete!"

      - name: Generate summary
        if: always()
        run: |
          echo "## Scrape Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Count exports
          if [ -d "exports/sites" ]; then
            SITE_COUNT=$(find exports/sites -type d -mindepth 1 -maxdepth 1 | wc -l)
            FILE_COUNT=$(find exports/sites -type f \( -name "*.csv" -o -name "*.xlsx" \) | wc -l)
            echo "### Raw Exports" >> $GITHUB_STEP_SUMMARY
            echo "- Sites scraped: $SITE_COUNT" >> $GITHUB_STEP_SUMMARY
            echo "- Files generated: $FILE_COUNT" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Count cleaned data
          if [ -d "exports/cleaned" ]; then
            CLEANED_COUNT=$(find exports/cleaned -type f \( -name "*.csv" -o -name "*.parquet" \) | wc -l)
            echo "### Cleaned Data" >> $GITHUB_STEP_SUMMARY
            echo "- Cleaned files: $CLEANED_COUNT" >> $GITHUB_STEP_SUMMARY

            # Check for master workbook
            if [ -f "exports/cleaned/MASTER_CLEANED_WORKBOOK.xlsx" ]; then
              SIZE=$(du -h exports/cleaned/MASTER_CLEANED_WORKBOOK.xlsx | cut -f1)
              echo "- Master workbook: Created ($SIZE)" >> $GITHUB_STEP_SUMMARY

              # Count records in master workbook
              RECORD_COUNT=$(python -c "import pandas as pd; print(len(pd.read_excel('exports/cleaned/MASTER_CLEANED_WORKBOOK.xlsx')))" 2>/dev/null || echo "Unknown")
              echo "- Total records: $RECORD_COUNT" >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Firestore upload status
          echo "### Firestore Upload" >> $GITHUB_STEP_SUMMARY
          if [ -f "exports/cleaned/MASTER_CLEANED_WORKBOOK.xlsx" ]; then
            echo "✅ Data uploaded to Firestore" >> $GITHUB_STEP_SUMMARY
            echo "- Collection: \`properties\`" >> $GITHUB_STEP_SUMMARY
            echo "- Queryable via: \`POST /api/firestore/query\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ No data to upload (master workbook not found)" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Show recent log entries
          if [ -f "logs/scraper.log" ]; then
            echo "### Recent Errors" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            grep -i "error\|warning" logs/scraper.log | tail -n 20 || echo "No errors found" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow:** #${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY

      - name: Upload raw exports as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-exports-raw-${{ github.run_number }}
          path: exports/sites/
          retention-days: 30
          if-no-files-found: warn

      - name: Upload cleaned data as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-exports-cleaned-${{ github.run_number }}
          path: exports/cleaned/
          retention-days: 30
          if-no-files-found: warn

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: logs/
          retention-days: 7
          if-no-files-found: ignore

      - name: Notify on failure
        if: failure()
        run: |
          echo "Scraper workflow failed!"
          echo "Check logs for details: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
