name: Nigerian Real Estate Scraper

on:
  # Trigger via frontend (repository_dispatch)
  repository_dispatch:
    types: [trigger-scrape]

  # Scheduled runs - Daily at 3 AM UTC (4 AM WAT during standard time, 5 AM WAT during DST)
  schedule:
    - cron: '0 3 * * *'

  # Manual trigger from GitHub Actions UI
  workflow_dispatch:
    inputs:
      page_cap:
        description: 'Max pages per site'
        required: false
        default: '20'
      geocode:
        description: 'Enable geocoding (1=yes, 0=no)'
        required: false
        default: '1'
      sites:
        description: 'Specific sites to scrape (comma-separated, leave empty for all enabled sites)'
        required: false
        default: ''

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60  # 1 hour max

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: ğŸ“¦ Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: ğŸ­ Install Playwright and browsers
        run: |
          playwright install chromium
          playwright install-deps chromium

      - name: âš™ï¸ Configure scraper
        run: |
          # Copy example config if config.yaml doesn't exist
          if [ ! -f config.yaml ]; then
            cp config.example.yaml config.yaml
            echo "âœ… Created config.yaml from example"
          fi

          # Enable default sites if no sites specified
          if [ -z "${{ github.event.inputs.sites }}" ] && [ -z "${{ github.event.client_payload.sites }}" ]; then
            echo "âœ… Using sites from config.yaml"
          else
            # Enable specific sites (from manual trigger or frontend)
            SITES="${{ github.event.inputs.sites }}${{ github.event.client_payload.sites }}"
            echo "âœ… Enabling sites: $SITES"
            # This would need a script to enable specific sites
            # For now, we'll use the config as-is
          fi

      - name: ğŸš€ Run scraper
        env:
          # Use inputs from either workflow_dispatch or repository_dispatch
          RP_PAGE_CAP: ${{ github.event.inputs.page_cap || github.event.client_payload.page_cap || '20' }}
          RP_GEOCODE: ${{ github.event.inputs.geocode || github.event.client_payload.geocode || '1' }}
          RP_HEADLESS: 1
          RP_NO_IMAGES: 1
          RP_DEBUG: 0
        run: |
          echo "ğŸ”§ Configuration:"
          echo "  - Page cap: $RP_PAGE_CAP"
          echo "  - Geocoding: $RP_GEOCODE"
          echo "  - Headless: $RP_HEADLESS"
          echo ""
          echo "ğŸƒ Starting scraper..."
          python main.py

      - name: ğŸ§¹ Process exports with watcher
        run: |
          echo "ğŸ“Š Processing scraped data..."
          python watcher.py --once

      - name: ğŸ“ Generate summary
        if: always()
        run: |
          echo "## ğŸ“Š Scrape Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Count exports
          if [ -d "exports/sites" ]; then
            SITE_COUNT=$(find exports/sites -type d -mindepth 1 -maxdepth 1 | wc -l)
            FILE_COUNT=$(find exports/sites -type f -name "*.csv" -o -name "*.xlsx" | wc -l)
            echo "### Raw Exports" >> $GITHUB_STEP_SUMMARY
            echo "- **Sites scraped:** $SITE_COUNT" >> $GITHUB_STEP_SUMMARY
            echo "- **Files generated:** $FILE_COUNT" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Count cleaned data
          if [ -d "exports/cleaned" ]; then
            CLEANED_COUNT=$(find exports/cleaned -type f -name "*.csv" -o -name "*.parquet" | wc -l)
            echo "### Cleaned Data" >> $GITHUB_STEP_SUMMARY
            echo "- **Cleaned files:** $CLEANED_COUNT" >> $GITHUB_STEP_SUMMARY

            # Check for master workbook
            if [ -f "exports/cleaned/MASTER_CLEANED_WORKBOOK.xlsx" ]; then
              SIZE=$(du -h exports/cleaned/MASTER_CLEANED_WORKBOOK.xlsx | cut -f1)
              echo "- **Master workbook:** âœ… ($SIZE)" >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Show recent log entries (last 20 lines, errors only)
          if [ -f "logs/scraper.log" ]; then
            echo "### Recent Errors" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            grep -i "error\|warning" logs/scraper.log | tail -n 20 || echo "No errors found âœ…" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow:** [\#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY

      - name: ğŸ“¤ Upload raw exports as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-exports-raw-${{ github.run_number }}
          path: |
            exports/sites/
          retention-days: 30
          if-no-files-found: warn

      - name: ğŸ“¤ Upload cleaned data as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-exports-cleaned-${{ github.run_number }}
          path: |
            exports/cleaned/
          retention-days: 30
          if-no-files-found: warn

      - name: ğŸ“¤ Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: |
            logs/
          retention-days: 7
          if-no-files-found: ignore

      - name: ğŸ”” Notify on failure
        if: failure()
        run: |
          echo "âŒ Scraper workflow failed!"
          echo "Check logs for details: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          # You can add email/Slack notification here
          # Example: curl -X POST webhook_url -d '{"text":"Scraper failed"}'
