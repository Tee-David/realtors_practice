name: Nigerian Real Estate Scraper

on:
  # Trigger via frontend (repository_dispatch)
  repository_dispatch:
    types: [trigger-scrape]

  # Manual trigger from GitHub Actions UI (you control when to run)
  workflow_dispatch:
    inputs:
      page_cap:
        description: 'Max pages per site'
        required: false
        default: '20'
      geocode:
        description: 'Enable geocoding (1=yes, 0=no)'
        required: false
        default: '1'
      sites:
        description: 'Specific sites to scrape (comma-separated, leave empty for all enabled sites)'
        required: false
        default: ''

jobs:
  detect-batch-size:
    runs-on: ubuntu-latest
    outputs:
      is_large_batch: ${{ steps.check.outputs.is_large_batch }}
      site_count: ${{ steps.check.outputs.site_count }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Detect batch size
        id: check
        run: |
          python - <<'EOF'
          import yaml
          import os

          # Load config
          with open('config.yaml') as f:
              config = yaml.safe_load(f)

          # Get sites parameter (from frontend trigger or manual input)
          sites_param = "${{ github.event.client_payload.sites || github.event.inputs.sites || '' }}"

          if sites_param:
              # Specific sites requested
              site_count = len([s.strip() for s in sites_param.split(',') if s.strip()])
          else:
              # All enabled sites
              site_count = sum(1 for site_config in config.get('sites', {}).values() if site_config.get('enabled', False))

          # Determine if large batch (>30 sites = use multi-session)
          is_large = site_count > 30

          # Output for GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"is_large_batch={'true' if is_large else 'false'}\n")
              f.write(f"site_count={site_count}\n")

          print(f"Detected {site_count} sites to scrape")
          print(f"Large batch mode: {'ENABLED' if is_large else 'DISABLED'}")
          EOF

  scrape:
    needs: detect-batch-size
    if: needs.detect-batch-size.outputs.is_large_batch == 'false'
    runs-on: ubuntu-latest
    timeout-minutes: 180

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright and browsers
        run: |
          playwright install chromium
          playwright install-deps chromium

      - name: Configure scraper
        run: |
          # Use config.yaml from repository
          if [ ! -f config.yaml ]; then
            echo "ERROR: config.yaml not found"
            exit 1
          fi
          echo "Using config.yaml from repository"

      - name: Run scraper
        env:
          RP_PAGE_CAP: ${{ github.event.inputs.page_cap || github.event.client_payload.page_cap || '20' }}
          RP_GEOCODE: ${{ github.event.inputs.geocode || github.event.client_payload.geocode || '1' }}
          RP_HEADLESS: 1
          RP_NO_IMAGES: 1
          RP_DEBUG: 0
        run: |
          echo "Configuration:"
          echo "  - Page cap: $RP_PAGE_CAP"
          echo "  - Geocoding: $RP_GEOCODE"
          echo "  - Headless: $RP_HEADLESS"
          echo ""
          echo "Starting scraper..."
          python main.py

      - name: Process exports with watcher
        run: |
          echo "Processing scraped data..."
          python watcher.py --once

      - name: Upload to Firestore
        env:
          FIREBASE_CREDENTIALS: ${{ secrets.FIREBASE_CREDENTIALS }}
        run: |
          echo "========================================="
          echo "Uploading data to Firestore..."
          echo "========================================="

          # Check if master workbook exists
          if [ ! -f "exports/cleaned/MASTER_CLEANED_WORKBOOK.xlsx" ]; then
            echo "âŒ ERROR: Master workbook not found!"
            echo "Watcher may have failed. Check previous step."
            exit 1
          fi

          # Check if Firebase credentials secret is set
          if [ -z "$FIREBASE_CREDENTIALS" ]; then
            echo "âŒ ERROR: FIREBASE_CREDENTIALS secret not set!"
            echo "Please add Firebase service account JSON to GitHub Secrets."
            exit 1
          fi

          # Create temporary credentials file from GitHub secret
          echo "$FIREBASE_CREDENTIALS" > firebase-temp-credentials.json

          # Validate credentials file is valid JSON
          if ! python -c "import json; json.load(open('firebase-temp-credentials.json'))" 2>/dev/null; then
            echo "âŒ ERROR: Invalid Firebase credentials JSON!"
            rm firebase-temp-credentials.json
            exit 1
          fi

          echo "âœ… Firebase credentials validated"

          # Set environment variable for the script
          export FIREBASE_SERVICE_ACCOUNT=firebase-temp-credentials.json

          # Run Firestore upload with cleanup enabled
          # Archives stale listings (>30 days old) to properties_archive collection
          echo "Running upload script with enterprise schema transformation..."
          if python scripts/upload_to_firestore.py --cleanup --max-age-days 30; then
            echo "âœ… Firestore upload completed successfully!"
          else
            echo "âŒ ERROR: Firestore upload failed!"
            rm firebase-temp-credentials.json
            exit 1
          fi

          # Clean up credentials file
          rm firebase-temp-credentials.json

          echo "========================================="
          echo "Firestore upload complete!"
          echo "========================================="

      - name: Send completion notification
        if: success()
        uses: actions/github-script@v7
        with:
          script: |
            // Count properties from master workbook
            const fs = require('fs');
            let propertyCount = 'Unknown';
            let siteCount = 'Unknown';

            try {
              const metadata = JSON.parse(fs.readFileSync('exports/cleaned/metadata.json', 'utf8'));
              propertyCount = metadata.total_records || 'Unknown';
              siteCount = metadata.site_count || 'Unknown';
            } catch (e) {
              console.log('Could not read metadata:', e.message);
            }

            // Create notification issue comment (if triggered via API, comment on issue)
            // Otherwise, just log the notification
            const notification = `
            ## âœ… Scrape Session Completed Successfully!

            **Workflow Run**: [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

            ### ðŸ“Š Results
            - **Sites Scraped**: ${siteCount}
            - **Properties Found**: ${propertyCount}
            - **Duration**: ~${{ github.job.duration || 'N/A' }} minutes
            - **Completed**: ${new Date().toISOString()}

            ### ðŸ“¥ Data Available
            - **Firestore**: Data uploaded and queryable via \`POST /api/firestore/query\`
            - **Artifacts**: Download exports from workflow run artifacts
            - **Master Workbook**: Available in cleaned exports artifact

            ### ðŸ”— Quick Links
            - [View Workflow Run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - [Download Artifacts](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}#artifacts)

            ---
            ðŸ¤– Automated notification from GitHub Actions
            `;

            console.log(notification);

            // If you want to send this to a specific GitHub issue or discussion, uncomment:
            // await github.rest.issues.createComment({
            //   owner: context.repo.owner,
            //   repo: context.repo.repo,
            //   issue_number: YOUR_ISSUE_NUMBER,
            //   body: notification
            // });

      - name: Generate summary
        if: always()
        run: |
          echo "## Scrape Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Count exports
          if [ -d "exports/sites" ]; then
            SITE_COUNT=$(find exports/sites -type d -mindepth 1 -maxdepth 1 | wc -l)
            FILE_COUNT=$(find exports/sites -type f \( -name "*.csv" -o -name "*.xlsx" \) | wc -l)
            echo "### Raw Exports" >> $GITHUB_STEP_SUMMARY
            echo "- Sites scraped: $SITE_COUNT" >> $GITHUB_STEP_SUMMARY
            echo "- Files generated: $FILE_COUNT" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Count cleaned data
          if [ -d "exports/cleaned" ]; then
            CLEANED_COUNT=$(find exports/cleaned -type f \( -name "*.csv" -o -name "*.parquet" \) | wc -l)
            echo "### Cleaned Data" >> $GITHUB_STEP_SUMMARY
            echo "- Cleaned files: $CLEANED_COUNT" >> $GITHUB_STEP_SUMMARY

            # Check for master workbook
            if [ -f "exports/cleaned/MASTER_CLEANED_WORKBOOK.xlsx" ]; then
              SIZE=$(du -h exports/cleaned/MASTER_CLEANED_WORKBOOK.xlsx | cut -f1)
              echo "- Master workbook: Created ($SIZE)" >> $GITHUB_STEP_SUMMARY

              # Count records in master workbook
              RECORD_COUNT=$(python -c "import pandas as pd; print(len(pd.read_excel('exports/cleaned/MASTER_CLEANED_WORKBOOK.xlsx')))" 2>/dev/null || echo "Unknown")
              echo "- Total records: $RECORD_COUNT" >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Firestore upload status
          echo "### Firestore Upload" >> $GITHUB_STEP_SUMMARY
          if [ -f "exports/cleaned/MASTER_CLEANED_WORKBOOK.xlsx" ]; then
            echo "âœ… Data uploaded to Firestore" >> $GITHUB_STEP_SUMMARY
            echo "- Collection: \`properties\`" >> $GITHUB_STEP_SUMMARY
            echo "- Queryable via: \`POST /api/firestore/query\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ No data to upload (master workbook not found)" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Show recent log entries
          if [ -f "logs/scraper.log" ]; then
            echo "### Recent Errors" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            grep -i "error\|warning" logs/scraper.log | tail -n 20 || echo "No errors found" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow:** #${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY

      - name: Upload raw exports as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-exports-raw-${{ github.run_number }}
          path: exports/sites/
          retention-days: 30
          if-no-files-found: warn

      - name: Upload cleaned data as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-exports-cleaned-${{ github.run_number }}
          path: exports/cleaned/
          retention-days: 30
          if-no-files-found: warn

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: logs/
          retention-days: 7
          if-no-files-found: ignore

      - name: Notify on failure
        if: failure()
        run: |
          echo "Scraper workflow failed!"
          echo "Check logs for details: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

  delegate-to-large-batch:
    needs: detect-batch-size
    if: needs.detect-batch-size.outputs.is_large_batch == 'true'
    runs-on: ubuntu-latest

    steps:
      - name: Notify large batch delegation
        run: |
          echo "ðŸš€ LARGE BATCH DETECTED: ${{ needs.detect-batch-size.outputs.site_count }} sites"
          echo "Delegating to multi-session workflow..."
          echo ""
          echo "The large batch workflow will:"
          echo "  - Split sites into sessions of 20"
          echo "  - Run 3 sessions in parallel"
          echo "  - Consolidate all results"
          echo "  - Upload to Firestore"
          echo ""
          echo "This prevents timeout and completes faster!"

      - name: Trigger large batch workflow
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'scrape-large-batch.yml',
              ref: 'main',
              inputs: {
                sites_per_session: '20',
                page_cap: '${{ github.event.client_payload.page_cap || github.event.inputs.page_cap || "20" }}',
                geocode: '${{ github.event.client_payload.geocode || github.event.inputs.geocode || "1" }}'
              }
            });

            console.log('âœ… Large batch workflow triggered successfully!');
            console.log('Check the "Large Batch Scraper (Multi-Session)" workflow for progress.');
