name: Production Scraper (Intelligent Auto-Batching)

on:
  # Trigger from frontend API
  repository_dispatch:
    types: [trigger-scrape]

  # Manual trigger from GitHub Actions UI
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Max pages per site (default: 8)'
        required: false
        default: '8'
      geocode:
        description: 'Enable geocoding (1=yes, 0=no)'
        required: false
        default: '1'
      sites:
        description: 'Specific sites to scrape (JSON array, e.g., ["npc","propertypro"]. Leave empty for all enabled sites)'
        required: false
        default: ''
      force_sites_per_session:
        description: 'Force specific sites per session (leave empty for auto-calculation)'
        required: false
        default: ''

jobs:
  calculate:
    name: Calculate Intelligent Batching
    runs-on: ubuntu-latest
    outputs:
      sessions: ${{ steps.batch.outputs.sessions }}
      total_sessions: ${{ steps.batch.outputs.total_sessions }}
      sites_per_session: ${{ steps.batch.outputs.sites_per_session }}
      estimated_minutes: ${{ steps.batch.outputs.estimated_minutes }}
      estimated_hours: ${{ steps.batch.outputs.estimated_hours }}
      is_safe: ${{ steps.batch.outputs.is_safe }}
      recommendation: ${{ steps.batch.outputs.recommendation }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml

      - name: Calculate intelligent batching strategy
        id: batch
        run: |
          python - <<'EOF'
          import yaml
          import json
          import os
          import math

          # Configuration - API sends strings, manual inputs are also strings
          MAX_PAGES_STR = "${{ github.event.inputs.max_pages || github.event.client_payload.max_pages || '15' }}"
          MAX_PAGES = int(MAX_PAGES_STR) if MAX_PAGES_STR else 15

          # Fix: Use explicit default instead of || to avoid falsy 0 bug
          GEOCODE_INPUT = "${{ github.event.inputs.geocode }}"
          GEOCODE_PAYLOAD = "${{ github.event.client_payload.geocode }}"
          GEOCODE_STR = GEOCODE_INPUT if GEOCODE_INPUT else (GEOCODE_PAYLOAD if GEOCODE_PAYLOAD else "1")
          GEOCODE = GEOCODE_STR

          FORCE_STR = "${{ github.event.inputs.force_sites_per_session || github.event.client_payload.sites_per_session || '' }}"
          FORCE_SITES_PER_SESSION = FORCE_STR if FORCE_STR else ""

          # Custom sites from frontend/manual trigger
          # API sends comma-separated string (e.g., "site1,site2,site3")
          CUSTOM_SITES_STR = "${{ github.event.client_payload.sites }}"
          CUSTOM_SITES_INPUT = "${{ github.event.inputs.sites }}"

          GEOCODE_ENABLED = GEOCODE == "1"

          # Log trigger source for debugging
          print("=== Workflow Trigger Info ===")
          print(f"Triggered by: ${{ github.event_name }}")
          print(f"Event action: ${{ github.event.action }}")
          print(f"Max pages: {MAX_PAGES}")
          print(f"Geocode: {GEOCODE} (enabled: {GEOCODE_ENABLED})")
          print(f"============================\n")

          # Time estimation constants (based on empirical testing)
          TIME_PER_PAGE = 8  # seconds
          TIME_PER_SITE_OVERHEAD = 45  # seconds
          GEOCODE_TIME_PER_PROPERTY = 1.2  # seconds
          FIRESTORE_UPLOAD_TIME = 0.3  # seconds
          WATCHER_OVERHEAD = 120  # seconds
          BUFFER_MULTIPLIER = 1.3  # 30% safety buffer

          # GitHub Actions constraints
          GITHUB_TIMEOUT_MINUTES = 350  # 6 hours minus 10 min buffer
          MAX_PARALLEL = 10  # Increased for detail scraping to fit within 6-hour limit

          # Force small batches for reliability (1 site per session - necessary for detail scraping)
          FORCED_SITES_PER_SESSION = 1

          # Load config.yaml
          with open('config.yaml') as f:
              config = yaml.safe_load(f)

          # Determine which sites to scrape
          # Priority: Custom sites from frontend/input > All enabled sites from config
          custom_sites = []

          # Try client_payload.sites first (from API trigger - comma-separated string)
          if CUSTOM_SITES_STR and CUSTOM_SITES_STR not in ['null', '', 'None']:
              sites_list = [s.strip() for s in CUSTOM_SITES_STR.split(',') if s.strip()]
              if sites_list:
                  custom_sites = sites_list
                  print(f"Using {len(custom_sites)} custom sites from API: {custom_sites}")

          # Try inputs.sites second (from manual workflow_dispatch)
          if not custom_sites and CUSTOM_SITES_INPUT and CUSTOM_SITES_INPUT not in ['null', '', 'None']:
              try:
                  # Try parsing as JSON first (for manual triggers)
                  parsed = json.loads(CUSTOM_SITES_INPUT)
                  if isinstance(parsed, list) and parsed:
                      custom_sites = parsed
                      print(f"Using custom sites from manual input: {custom_sites}")
              except:
                  # Fall back to comma-separated
                  sites_list = [s.strip() for s in CUSTOM_SITES_INPUT.split(',') if s.strip()]
                  if sites_list:
                      custom_sites = sites_list
                      print(f"Using custom sites (CSV): {custom_sites}")

          # Validate custom sites exist in config
          if custom_sites:
              all_sites = list(config.get('sites', {}).keys())
              valid_sites = [s for s in custom_sites if s in all_sites]
              invalid_sites = [s for s in custom_sites if s not in all_sites]

              if invalid_sites:
                  print(f"WARNING: Invalid sites (not in config): {invalid_sites}")

              if valid_sites:
                  enabled_sites = valid_sites
                  print(f"SUCCESS: Validated {len(valid_sites)} custom sites")
              else:
                  print(f"ERROR: No valid sites found in custom list. Using all enabled sites.")
                  enabled_sites = [
                      site_id for site_id, site_config in config.get('sites', {}).items()
                      if site_config.get('enabled', False)
                  ]
          else:
              # No custom sites - use all enabled from config
              enabled_sites = [
                  site_id for site_id, site_config in config.get('sites', {}).items()
                  if site_config.get('enabled', False)
              ]
              print(f"Using all enabled sites from config.yaml")

          total_sites = len(enabled_sites)
          print(f"Total sites to scrape: {total_sites}")
          print(f"Sites: {enabled_sites}")
          print(f"Max pages per site: {MAX_PAGES}")
          print(f"Geocoding: {GEOCODE_ENABLED}")

          # Estimate time for single site
          scrape_time = (MAX_PAGES * TIME_PER_PAGE) + TIME_PER_SITE_OVERHEAD
          estimated_properties = MAX_PAGES * 15
          geocode_time = estimated_properties * GEOCODE_TIME_PER_PROPERTY if GEOCODE_ENABLED else 0
          upload_time = estimated_properties * FIRESTORE_UPLOAD_TIME
          time_per_site = scrape_time + geocode_time + upload_time

          print(f"Estimated time per site: {time_per_site:.0f}s ({time_per_site/60:.1f} min)")

          # Calculate optimal sites per session
          # Priority: User input > Forced default > Auto-calculate
          if FORCE_STR:
              sites_per_session = int(FORCE_STR)
              print(f"Using user-specified sites_per_session: {sites_per_session}")
          elif FORCED_SITES_PER_SESSION:
              sites_per_session = FORCED_SITES_PER_SESSION
              print(f"Using fixed sites_per_session for reliability: {sites_per_session}")
          else:
              # Auto-calculate to stay under timeout
              max_sites_per_session = int(
                  (GITHUB_TIMEOUT_MINUTES * 60) / BUFFER_MULTIPLIER / time_per_site
              )
              sites_per_session = max(1, max_sites_per_session)
              print(f"Auto-calculated sites_per_session: {sites_per_session}")

          # Calculate sessions
          total_sessions = math.ceil(total_sites / sites_per_session)

          # Create session batches
          sessions = []
          for i in range(total_sessions):
              start = i * sites_per_session
              end = min(start + sites_per_session, total_sites)
              batch = enabled_sites[start:end]
              sessions.append({
                  'session_id': i + 1,
                  'sites': ','.join(batch),
                  'site_count': len(batch)
              })

          # Estimate total time
          session_time = (time_per_site * sites_per_session + WATCHER_OVERHEAD) * BUFFER_MULTIPLIER
          total_time_minutes = (session_time / 60) * total_sessions / MAX_PARALLEL
          total_time_hours = total_time_minutes / 60

          is_safe = total_time_minutes < GITHUB_TIMEOUT_MINUTES

          # Generate recommendation
          if not is_safe:
              recommendation = f"WARNING: Estimated {total_time_minutes:.0f}min exceeds {GITHUB_TIMEOUT_MINUTES}min limit. Reduce to {sites_per_session-5} sites/session"
          elif total_time_minutes > GITHUB_TIMEOUT_MINUTES * 0.9:
              recommendation = f"CLOSE TO LIMIT: {total_time_minutes:.0f}min. Consider {sites_per_session-5} sites/session for safety"
          else:
              recommendation = f"SAFE: {total_time_minutes:.0f}min well within {GITHUB_TIMEOUT_MINUTES}min limit"

          print(f"\nBatching Strategy:")
          print(f"  Sites per session: {sites_per_session}")
          print(f"  Total sessions: {total_sessions}")
          print(f"  Estimated time: {total_time_minutes:.1f} min ({total_time_hours:.2f} hours)")
          print(f"  Safety status: {recommendation}")

          # Output for next job
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"sessions={json.dumps(sessions)}\n")
              f.write(f"total_sessions={total_sessions}\n")
              f.write(f"sites_per_session={sites_per_session}\n")
              f.write(f"estimated_minutes={total_time_minutes:.1f}\n")
              f.write(f"estimated_hours={total_time_hours:.2f}\n")
              f.write(f"is_safe={str(is_safe).lower()}\n")
              f.write(f"recommendation={recommendation}\n")

          # Summary
          print("\nSession breakdown:")
          for session in sessions:
              print(f"  Session {session['session_id']}: {session['site_count']} sites")
          EOF

      - name: Display strategy summary
        run: |
          echo "## Intelligent Batching Strategy" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Configuration:**" >> $GITHUB_STEP_SUMMARY
          echo "- Max pages per site: ${{ github.event.inputs.max_pages || github.event.client_payload.max_pages || '8' }}" >> $GITHUB_STEP_SUMMARY
          echo "- Geocoding: ${{ steps.batch.outputs.is_safe == 'true' && 'SUCCESS: Enabled' || 'ERROR: Disabled' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Batching:**" >> $GITHUB_STEP_SUMMARY
          echo "- Sites per session: ${{ steps.batch.outputs.sites_per_session }}" >> $GITHUB_STEP_SUMMARY
          echo "- Total sessions: ${{ steps.batch.outputs.total_sessions }}" >> $GITHUB_STEP_SUMMARY
          echo "- Max parallel: 10" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Time Estimate:**" >> $GITHUB_STEP_SUMMARY
          echo "- Total: ${{ steps.batch.outputs.estimated_minutes }} min (${{ steps.batch.outputs.estimated_hours }} hours)" >> $GITHUB_STEP_SUMMARY
          echo "- Status: ${{ steps.batch.outputs.is_safe == 'true' && 'SUCCESS:' || 'WARNING:' }} ${{ steps.batch.outputs.recommendation }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

  scrape:
    name: Scrape Session ${{ matrix.session.session_id }}
    needs: calculate
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2 hours per session (1 site with detail scraping = ~80 min estimated)
    strategy:
      max-parallel: 10  # Increased for detail scraping to fit within 6-hour limit
      fail-fast: false
      matrix:
        session: ${{ fromJson(needs.calculate.outputs.sessions) }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright and browsers
        run: |
          playwright install chromium
          playwright install-deps chromium

      - name: Run scraper (Session ${{ matrix.session.session_id }}/${{ needs.calculate.outputs.total_sessions }})
        env:
          FIREBASE_CREDENTIALS: ${{ secrets.FIREBASE_CREDENTIALS }}
          FIRESTORE_ENABLED: "1"
          RP_PAGE_CAP: ${{ github.event.inputs.max_pages || github.event.client_payload.max_pages || '8' }}
          RP_GEOCODE: ${{ github.event.inputs.geocode || github.event.client_payload.geocode || '1' }}
          RP_HEADLESS: 1
          RP_NO_IMAGES: 1
          RP_DEBUG: 0
        run: |
          echo "=== Session ${{ matrix.session.session_id }}/${{ needs.calculate.outputs.total_sessions }} ==="
          echo "Sites in this session: ${{ matrix.session.site_count }}"
          echo "Sites per session (optimal): ${{ needs.calculate.outputs.sites_per_session }}"
          echo "Page cap: $RP_PAGE_CAP"
          echo "Geocoding: $RP_GEOCODE"
          echo "Estimated session time: ${{ needs.calculate.outputs.estimated_minutes }} min"
          echo ""

          # Set up Firebase credentials
          if [ ! -z "$FIREBASE_CREDENTIALS" ]; then
            echo "$FIREBASE_CREDENTIALS" > firebase-temp-credentials.json
            export FIREBASE_SERVICE_ACCOUNT=firebase-temp-credentials.json
            echo "SUCCESS: Firebase credentials configured"
          fi

          # Enable only this session's sites
          SITES="${{ matrix.session.sites }}"
          SITES_SPACE=$(echo "$SITES" | tr ',' ' ')
          echo "Enabling sites: $SITES_SPACE"
          python scripts/enable_sites.py $SITES_SPACE

          # Run scraper
          python main.py

          # Cleanup
          if [ -f "firebase-temp-credentials.json" ]; then
            rm firebase-temp-credentials.json
          fi

      - name: Process exports
        run: |
          echo "Processing scraped data..."
          python watcher.py --once

      - name: Upload session artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: session-${{ matrix.session.session_id }}-exports
          path: exports/
          retention-days: 7

  consolidate:
    name: Consolidate All Sessions
    needs: [calculate, scrape]
    if: ${{ always() }}  # Run even if some scrape sessions timeout/fail - prevents data loss
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download all session exports
        uses: actions/download-artifact@v4
        with:
          path: session-exports/
          pattern: session-*-exports
          merge-multiple: false
        continue-on-error: true  # Don't fail if no artifacts (all scrape sessions might have failed)

      - name: Consolidate sessions
        run: |
          echo "Consolidating ${{ needs.calculate.outputs.total_sessions }} sessions..."
          mkdir -p exports/sites exports/cleaned

          python - <<'EOF'
          import os
          import sys
          import shutil
          from pathlib import Path

          session_dir = Path('session-exports')
          consolidated_dir = Path('exports/sites')

          # Check if session-exports directory exists and has content
          if not session_dir.exists():
              print("ERROR: session-exports directory not found!")
              print("This means no artifacts were downloaded (all scrape sessions may have failed)")
              sys.exit(1)

          # Count artifacts
          session_folders = list(session_dir.glob('session-*-exports'))
          if not session_folders:
              print("ERROR: No session artifact folders found!")
              print(f"Checked pattern: session-*-exports in {session_dir}")
              print(f"Directory contents:")
              for item in session_dir.iterdir():
                  print(f"  - {item.name}")
              sys.exit(1)

          print(f"Found {len(session_folders)} session artifact folders")

          files_copied = 0
          for session_folder in session_folders:
              session_sites = session_folder / 'sites'
              if session_sites.exists():
                  for site_folder in session_sites.iterdir():
                      if site_folder.is_dir():
                          dest = consolidated_dir / site_folder.name
                          dest.mkdir(exist_ok=True)
                          for file in site_folder.iterdir():
                              shutil.copy2(file, dest / file.name)
                              files_copied += 1
                              print(f"Copied: {file.name}")
              else:
                  print(f"WARNING: No 'sites' folder in {session_folder.name}")

          print(f"\nConsolidation complete!")
          print(f"Total files copied: {files_copied}")
          print(f"Destination: {consolidated_dir}")

          if files_copied == 0:
              print("ERROR: No files were consolidated!")
              print("This means scraping sessions produced no export files")
              sys.exit(1)
          EOF

      - name: Create master workbook (OPTIONAL - DISABLED BY DEFAULT)
        if: env.MASTER_WORKBOOK_ENABLED == '1'
        run: |
          echo "Creating master workbook..."
          echo "NOTE: Master workbook is optional - Firestore is primary data store"
          python watcher.py --once

      - name: Upload to Firestore (Direct from Sessions)
        env:
          FIREBASE_CREDENTIALS: ${{ secrets.FIREBASE_CREDENTIALS }}
        run: |
          echo "========================================="
          echo "Uploading to Firestore (Enterprise Schema v3.1)"
          echo "Direct upload from session exports (no master workbook required)"
          echo "========================================="

          # Check if Firebase credentials secret is set (HARD FAIL if not)
          if [ -z "$FIREBASE_CREDENTIALS" ]; then
            echo "ERROR: FIREBASE_CREDENTIALS secret not set!"
            echo "Go to: Settings → Secrets and variables → Actions"
            echo "Add secret 'FIREBASE_CREDENTIALS' with Firebase service account JSON"
            exit 1
          fi

          # Create temporary credentials file
          echo "$FIREBASE_CREDENTIALS" > firebase-temp-credentials.json

          # Validate credentials file is valid JSON (HARD FAIL if invalid)
          if ! python -c "import json; json.load(open('firebase-temp-credentials.json'))" 2>/dev/null; then
            echo "ERROR: Invalid Firebase credentials JSON!"
            echo "Check that FIREBASE_CREDENTIALS secret contains valid JSON"
            rm firebase-temp-credentials.json
            exit 1
          fi

          echo "SUCCESS: Firebase credentials validated"

          # Set environment variables
          export FIREBASE_SERVICE_ACCOUNT=firebase-temp-credentials.json
          export FIRESTORE_ENABLED=1

          # Verify exports/sites directory exists
          if [ ! -d "exports/sites" ]; then
            echo "ERROR: exports/sites directory does not exist!"
            echo "Consolidation step may have failed. Check logs above."
            rm -f firebase-temp-credentials.json
            exit 1
          fi

          # Count session export files
          echo ""
          echo "Counting session export files..."
          csv_count=$(find exports/sites -name "*.csv" 2>/dev/null | wc -l)
          xlsx_count=$(find exports/sites -name "*.xlsx" 2>/dev/null | wc -l)
          total_files=$((csv_count + xlsx_count))
          echo "Found $total_files export files ($csv_count CSV + $xlsx_count XLSX)"

          if [ $total_files -eq 0 ]; then
            echo "ERROR: No export files found in exports/sites!"
            echo "Consolidation step may have completed but produced no files."
            echo ""
            echo "Directory structure:"
            ls -la exports/ 2>/dev/null || echo "exports/ does not exist"
            ls -la exports/sites/ 2>/dev/null || echo "exports/sites/ does not exist"
            echo ""
            echo "Possible causes:"
            echo "  1. All scrape sessions failed or timed out"
            echo "  2. Scraping succeeded but produced no data"
            echo "  3. Consolidation script failed to copy files"
            rm -f firebase-temp-credentials.json
            exit 1
          fi

          # Run NEW upload script (uploads directly from session exports)
          echo ""
          echo "Uploading with enterprise schema (9 categories, 85+ fields)..."
          echo "- Auto-detection: listing_type, furnishing, condition"
          echo "- Auto-tagging: premium, hot_deal"
          echo "- Location intelligence: 50+ Lagos landmarks"
          echo ""

          if python scripts/upload_sessions_to_firestore.py; then
            echo ""
            echo "SUCCESS: Firestore upload completed successfully!"
          else
            echo "ERROR: Firestore upload failed!"
            echo "Check logs above for details"
            rm -f firebase-temp-credentials.json
            exit 1
          fi

          # Clean up credentials
          rm -f firebase-temp-credentials.json

          echo "========================================="
          echo "Firestore upload step complete!"
          echo "========================================="

      - name: Verify Firestore Upload
        env:
          FIREBASE_CREDENTIALS: ${{ secrets.FIREBASE_CREDENTIALS }}
        run: |
          echo "Verifying Firestore upload..."

          # Recreate credentials file for verification
          echo "$FIREBASE_CREDENTIALS" > firebase-temp-credentials.json
          export FIREBASE_SERVICE_ACCOUNT=firebase-temp-credentials.json

          # Run verification script
          if python scripts/verify_firestore_upload.py; then
            echo "[SUCCESS] Firestore upload verified!"
          else
            echo "[WARNING] Verification failed - check Firebase Console manually"
            echo "Workflow will continue (verification is non-blocking)"
          fi

          # Clean up
          rm -f firebase-temp-credentials.json

      - name: Generate final summary
        if: always()
        run: |
          echo "## Production Scrape Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Intelligent Batching:**" >> $GITHUB_STEP_SUMMARY
          echo "- Strategy: ${{ needs.calculate.outputs.recommendation }}" >> $GITHUB_STEP_SUMMARY
          echo "- Sessions completed: ${{ needs.calculate.outputs.total_sessions }}" >> $GITHUB_STEP_SUMMARY
          echo "- Sites per session: ${{ needs.calculate.outputs.sites_per_session }}" >> $GITHUB_STEP_SUMMARY
          echo "- Estimated time: ${{ needs.calculate.outputs.estimated_hours }} hours" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -d "exports/sites" ]; then
            SITE_COUNT=$(find exports/sites -type d -mindepth 1 -maxdepth 1 | wc -l)
            FILE_COUNT=$(find exports/sites -type f \( -name "*.csv" -o -name "*.xlsx" \) | wc -l)
            echo "**Results:**" >> $GITHUB_STEP_SUMMARY
            echo "- Sites scraped: $SITE_COUNT" >> $GITHUB_STEP_SUMMARY
            echo "- Files generated: $FILE_COUNT" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          if [ -f "exports/cleaned/MASTER_CLEANED_WORKBOOK.xlsx" ]; then
            SIZE=$(du -h exports/cleaned/MASTER_CLEANED_WORKBOOK.xlsx | cut -f1)
            RECORD_COUNT=$(python -c "import pandas as pd; print(len(pd.read_excel('exports/cleaned/MASTER_CLEANED_WORKBOOK.xlsx')))" 2>/dev/null || echo "Unknown")
            echo "**Master Workbook:**" >> $GITHUB_STEP_SUMMARY
            echo "- Size: $SIZE" >> $GITHUB_STEP_SUMMARY
            echo "- Properties: $RECORD_COUNT" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Firestore upload status
          FIRESTORE_COUNT=$(grep -c "uploaded successfully" firestore_upload.log 2>/dev/null || echo "0")

          echo "**Firestore:**" >> $GITHUB_STEP_SUMMARY
          if [ "$FIRESTORE_COUNT" -gt 0 ]; then
            echo "SUCCESS: Uploaded $FIRESTORE_COUNT properties successfully" >> $GITHUB_STEP_SUMMARY
          else
            echo "WARNING: No properties uploaded (check logs)" >> $GITHUB_STEP_SUMMARY
          fi
          echo "- Collection: \`properties\`" >> $GITHUB_STEP_SUMMARY
          echo "- Schema: Enterprise v3.1 (9 categories, 85+ fields)" >> $GITHUB_STEP_SUMMARY
          echo "- Location: Lagos area only" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow:** #${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date -u '+%Y-%m-%d %H:%M UTC')" >> $GITHUB_STEP_SUMMARY

      - name: Upload consolidated exports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-exports-${{ github.run_number }}
          path: exports/cleaned/
          retention-days: 30
